{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGRyTem-FnyP",
        "outputId": "13db06e4-1429-4c7b-d4b5-8b4e73ef892f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'modulus-makani'...\n",
            "remote: Enumerating objects: 445, done.\u001b[K\n",
            "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 445 (delta 113), reused 97 (delta 77), pack-reused 249 (from 1)\u001b[K\n",
            "Receiving objects: 100% (445/445), 35.43 MiB | 15.99 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/modulus-makani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWM4_EQxF1js",
        "outputId": "0f3e776d-63c0-4034-87f2-85ac80619d02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "modulus-makani\tsample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYnl1mqHFtnh",
        "outputId": "815e0548-4fd0-4fdb-dd51-bf92296fcacb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from makani==0.1.0) (2.4.1+cu121)\n",
            "Collecting numpy<1.25,>=1.22.4 (from makani==0.1.0)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: numba>=0.50.0 in /usr/local/lib/python3.10/dist-packages (from makani==0.1.0) (0.60.0)\n",
            "Collecting nvidia-dali-cuda110>=1.16.0 (from makani==0.1.0)\n",
            "  Downloading nvidia_dali_cuda110-1.41.0.tar.gz (1.5 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-modulus>=0.5.0a0 (from makani==0.1.0)\n",
            "  Downloading nvidia_modulus-0.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torch-harmonics>=0.6.5 (from makani==0.1.0)\n",
            "  Downloading torch_harmonics-0.7.1.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorly>=0.8.1 (from makani==0.1.0)\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting tensorly-torch>=0.4.0 (from makani==0.1.0)\n",
            "  Downloading tensorly_torch-0.5.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.50.0->makani==0.1.0) (0.43.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110>=1.16.0->makani==0.1.0) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110>=1.16.0->makani==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110>=1.16.0->makani==0.1.0) (0.1.8)\n",
            "Collecting nvidia-nvimgcodec-cu11>=0.2.0 (from nvidia-dali-cuda110>=1.16.0->makani==0.1.0)\n",
            "  Downloading nvidia_nvimgcodec_cu11-0.3.0.5-py3-none-manylinux2014_x86_64.whl.metadata (761 bytes)\n",
            "Requirement already satisfied: xarray>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus>=0.5.0a0->makani==0.1.0) (2024.9.0)\n",
            "Collecting zarr>=2.14.2 (from nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading zarr-2.18.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: fsspec>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus>=0.5.0a0->makani==0.1.0) (2024.6.1)\n",
            "Collecting s3fs>=2023.5.0 (from nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading s3fs-2024.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-dali-cuda120>=1.35.0 (from nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading nvidia_dali_cuda120-1.41.0.tar.gz (1.5 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=67.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus>=0.5.0a0->makani==0.1.0) (71.0.4)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus>=0.5.0a0->makani==0.1.0) (2024.8.30)\n",
            "Requirement already satisfied: pytz>=2023.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus>=0.5.0a0->makani==0.1.0) (2024.2)\n",
            "Collecting treelib>=1.2.5 (from nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading treelib-1.7.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus>=0.5.0a0->makani==0.1.0) (4.66.5)\n",
            "Requirement already satisfied: nvtx>=0.2.8 in /usr/local/lib/python3.10/dist-packages (from nvidia-modulus>=0.5.0a0->makani==0.1.0) (0.2.10)\n",
            "Collecting onnx>=1.14.0 (from nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting timm>=0.9.12 (from nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorly>=0.8.1->makani==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->makani==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->makani==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->makani==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->makani==0.1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->makani==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda110>=1.16.0->makani==0.1.0) (0.44.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda110>=1.16.0->makani==0.1.0) (1.16.0)\n",
            "Collecting nvidia-nvimgcodec-cu12>=0.2.0 (from nvidia-dali-cuda120>=1.35.0->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading nvidia_nvimgcodec_cu12-0.3.0.5-py3-none-manylinux2014_x86_64.whl.metadata (761 bytes)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.14.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (3.20.3)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading aiobotocore-2.15.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting fsspec>=2023.1.0 (from nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (3.10.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.12->nvidia-modulus>=0.5.0a0->makani==0.1.0) (0.19.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.12->nvidia-modulus>=0.5.0a0->makani==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.12->nvidia-modulus>=0.5.0a0->makani==0.1.0) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9.12->nvidia-modulus>=0.5.0a0->makani==0.1.0) (0.4.5)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from xarray>=2023.1.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (24.1)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from xarray>=2023.1.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (2.1.4)\n",
            "Collecting asciitree (from zarr>=2.14.2->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numcodecs>=0.10.0 (from zarr>=2.14.2->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading numcodecs-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting fasteners (from zarr>=2.14.2->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->makani==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->makani==0.1.0) (1.3.0)\n",
            "Collecting botocore<1.35.24,>=1.35.16 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading botocore-1.35.23-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (1.16.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray>=2023.1.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->xarray>=2023.1.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (2024.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm>=0.9.12->nvidia-modulus>=0.5.0a0->makani==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm>=0.9.12->nvidia-modulus>=0.5.0a0->makani==0.1.0) (10.4.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.35.24,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.24,>=1.35.16->aiobotocore<3.0.0,>=2.5.4->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2023.5.0->nvidia-modulus>=0.5.0a0->makani==0.1.0) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm>=0.9.12->nvidia-modulus>=0.5.0a0->makani==0.1.0) (3.3.2)\n",
            "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_modulus-0.8.0-py3-none-any.whl (498 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.9/498.9 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorly_torch-0.5.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.3/59.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvimgcodec_cu11-0.3.0.5-py3-none-manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3fs-2024.9.0-py3-none-any.whl (29 kB)\n",
            "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading treelib-1.7.0-py3-none-any.whl (18 kB)\n",
            "Downloading zarr-2.18.3-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiobotocore-2.15.1-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numcodecs-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvimgcodec_cu12-0.3.0.5-py3-none-manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.35.23-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: makani, nvidia-dali-cuda110, torch-harmonics, nvidia-dali-cuda120, asciitree\n",
            "  Building editable for makani (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for makani: filename=makani-0.1.0-0.editable-py3-none-any.whl size=9888 sha256=631eb3ca944f39c0da78197f1a288d45fc942edce5c32877c067e70cfd5066de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-99povlg5/wheels/e8/d3/96/0e8c7135806cbda4db28d12fc8d710e5e4f66ced1411163e67\n",
            "  Building wheel for nvidia-dali-cuda110 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-dali-cuda110: filename=nvidia_dali_cuda110-1.41.0-17427118-py3-none-manylinux2014_x86_64.whl size=508923585 sha256=6b12993384b694463c651a6c22621e6982b8834946eefcc864ab061b5c6e972e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/a7/cc/343f747a56b1d82c3186919070cb11796f1c7390c57a9ff715\n",
            "  Building wheel for torch-harmonics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-harmonics: filename=torch_harmonics-0.7.1-cp310-cp310-linux_x86_64.whl size=8690038 sha256=e8a11ebf9950360ac645fb8bd6a8677f28dd431f7b92972a17f36cbde1a34277\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/76/e1/10309bbf7a94e285bda8cca87b77d12df0f33b6c34d228fb9b\n",
            "  Building wheel for nvidia-dali-cuda120 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-dali-cuda120: filename=nvidia_dali_cuda120-1.41.0-17427117-py3-none-manylinux2014_x86_64.whl size=311007621 sha256=240b4135e7c71c5f669d2f2970fa350f7ad1a0a4aab588a3ced578f9b6d7abd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/c5/9c/00b100f32961fa2537f9742fc2f5ae5c4f21bc7f22879ae707\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5033 sha256=ec4dcced6c5cc107d50b17d24704d1c5884e9af1325b5d1adc1ff149b2f99dc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\n",
            "Successfully built makani nvidia-dali-cuda110 torch-harmonics nvidia-dali-cuda120 asciitree\n",
            "Installing collected packages: asciitree, treelib, nvidia-nvimgcodec-cu12, nvidia-nvimgcodec-cu11, numpy, jmespath, fsspec, fasteners, aioitertools, onnx, nvidia-dali-cuda120, nvidia-dali-cuda110, numcodecs, botocore, zarr, torch-harmonics, tensorly-torch, tensorly, timm, aiobotocore, s3fs, nvidia-modulus, makani\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n"
          ]
        }
      ],
      "source": [
        "!mv modulus-makani/* .\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IabN_3iHrtX"
      },
      "outputs": [],
      "source": [
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhPRmrEGHwTx"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWf6o1KxH6ce"
      },
      "outputs": [],
      "source": [
        "!pip install ruamel.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se6RZky6YxXQ"
      },
      "source": [
        "# SFNO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vlf-d8cWF5L9"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.cuda import amp\n",
        "\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2Xr43m2HfT3"
      },
      "outputs": [],
      "source": [
        "# helpers\n",
        "from makani.models.common import DropPath, MLP, EncoderDecoder\n",
        "\n",
        "# import global convolution and non-linear spectral layers\n",
        "from makani.models.common import SpectralConv, FactorizedSpectralConv, SpectralAttention\n",
        "\n",
        "# get spectral transforms from torch_harmonics\n",
        "import torch_harmonics as th\n",
        "import torch_harmonics.distributed as thd\n",
        "\n",
        "# wrap fft, to unify interface to spectral transforms\n",
        "from makani.models.common import RealFFT2, InverseRealFFT2\n",
        "from makani.mpu.layers import DistributedRealFFT2, DistributedInverseRealFFT2, DistributedMLP, DistributedEncoderDecoder\n",
        "\n",
        "# more distributed stuff\n",
        "from makani.utils import comm\n",
        "\n",
        "# layer normalization\n",
        "from modulus.distributed.mappings import scatter_to_parallel_region, gather_from_parallel_region\n",
        "from makani.mpu.layer_norm import DistributedInstanceNorm2d, DistributedLayerNorm\n",
        "\n",
        "# for annotation of models\n",
        "import modulus\n",
        "from modulus.models.meta import ModelMetaData\n",
        "from modulus.distributed.utils import compute_split_shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGEq0ShHHka9"
      },
      "outputs": [],
      "source": [
        "class SpectralFilterLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        forward_transform,\n",
        "        inverse_transform,\n",
        "        embed_dim,\n",
        "        filter_type=\"linear\",\n",
        "        operator_type=\"diagonal\",\n",
        "        hidden_size_factor=1,\n",
        "        factorization=None,\n",
        "        rank=1.0,\n",
        "        separable=False,\n",
        "        complex_activation=\"real\",\n",
        "        spectral_layers=1,\n",
        "        bias=False,\n",
        "        drop_rate=0.0,\n",
        "        gain=1.0,\n",
        "    ):\n",
        "        super(SpectralFilterLayer, self).__init__()\n",
        "\n",
        "        if filter_type == \"non-linear\":\n",
        "            self.filter = SpectralAttention(\n",
        "                forward_transform,\n",
        "                inverse_transform,\n",
        "                embed_dim,\n",
        "                embed_dim,\n",
        "                operator_type=operator_type,\n",
        "                hidden_size_factor=hidden_size_factor,\n",
        "                complex_activation=complex_activation,\n",
        "                spectral_layers=spectral_layers,\n",
        "                drop_rate=drop_rate,\n",
        "                bias=bias,\n",
        "                gain=gain,\n",
        "            )\n",
        "\n",
        "        elif filter_type == \"linear\" and factorization is None:\n",
        "            self.filter = SpectralConv(\n",
        "                forward_transform,\n",
        "                inverse_transform,\n",
        "                embed_dim,\n",
        "                embed_dim,\n",
        "                operator_type=operator_type,\n",
        "                separable=separable,\n",
        "                bias=bias,\n",
        "                gain=gain,\n",
        "            )\n",
        "\n",
        "        elif filter_type == \"linear\" and factorization is not None:\n",
        "            self.filter = FactorizedSpectralConv(\n",
        "                forward_transform,\n",
        "                inverse_transform,\n",
        "                embed_dim,\n",
        "                embed_dim,\n",
        "                operator_type=operator_type,\n",
        "                rank=rank,\n",
        "                factorization=factorization,\n",
        "                separable=separable,\n",
        "                bias=bias,\n",
        "                gain=gain,\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise (NotImplementedError)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.filter(x)\n",
        "\n",
        "\n",
        "class FourierNeuralOperatorBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        forward_transform,\n",
        "        inverse_transform,\n",
        "        embed_dim,\n",
        "        filter_type=\"linear\",\n",
        "        operator_type=\"diagonal\",\n",
        "        mlp_ratio=2.0,\n",
        "        mlp_drop_rate=0.0,\n",
        "        path_drop_rate=0.0,\n",
        "        act_layer=nn.GELU,\n",
        "        norm_layer=(nn.Identity, nn.Identity),\n",
        "        rank=1.0,\n",
        "        factorization=None,\n",
        "        separable=False,\n",
        "        inner_skip=\"linear\",\n",
        "        outer_skip=None,\n",
        "        use_mlp=False,\n",
        "        comm_feature_inp_name=None,\n",
        "        comm_feature_hidden_name=None,\n",
        "        complex_activation=\"real\",\n",
        "        spectral_layers=1,\n",
        "        bias=False,\n",
        "        final_activation=False,\n",
        "        checkpointing=0,\n",
        "    ):\n",
        "        super(FourierNeuralOperatorBlock, self).__init__()\n",
        "\n",
        "        # determine some shapes\n",
        "        if comm.get_size(\"spatial\") > 1:\n",
        "            self.input_shape_loc = (forward_transform.lat_shapes[comm.get_rank(\"h\")],\n",
        "                                    forward_transform.lon_shapes[comm.get_rank(\"w\")])\n",
        "            self.output_shape_loc = (inverse_transform.lat_shapes[comm.get_rank(\"h\")],\n",
        "                                     inverse_transform.lon_shapes[comm.get_rank(\"w\")])\n",
        "        else:\n",
        "            self.input_shape_loc = (forward_transform.nlat, forward_transform.nlon)\n",
        "            self.output_shape_loc = (inverse_transform.nlat, inverse_transform.nlon)\n",
        "\n",
        "        # norm layer\n",
        "        self.norm0 = norm_layer[0]()\n",
        "\n",
        "        if act_layer == nn.Identity:\n",
        "            gain_factor = 1.0\n",
        "        else:\n",
        "            gain_factor = 2.0\n",
        "\n",
        "        if inner_skip == \"linear\":\n",
        "            self.inner_skip = nn.Conv2d(embed_dim, embed_dim, 1, 1, bias=False)\n",
        "            gain_factor /= 2.0\n",
        "            nn.init.normal_(self.inner_skip.weight, std=math.sqrt(gain_factor / embed_dim))\n",
        "        elif inner_skip == \"identity\":\n",
        "            self.inner_skip = nn.Identity()\n",
        "            gain_factor /= 2.0\n",
        "        elif inner_skip == \"none\":\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown skip connection type {inner_skip}\")\n",
        "\n",
        "        # convolution layer\n",
        "        self.filter = SpectralFilterLayer(\n",
        "            forward_transform,\n",
        "            inverse_transform,\n",
        "            embed_dim,\n",
        "            filter_type,\n",
        "            operator_type,\n",
        "            hidden_size_factor=mlp_ratio,\n",
        "            factorization=factorization,\n",
        "            rank=rank,\n",
        "            separable=separable,\n",
        "            complex_activation=complex_activation,\n",
        "            spectral_layers=spectral_layers,\n",
        "            bias=bias,\n",
        "            drop_rate=path_drop_rate,\n",
        "            gain=gain_factor,\n",
        "        )\n",
        "\n",
        "        self.act_layer0 = act_layer()\n",
        "\n",
        "        # norm layer\n",
        "        self.norm1 = norm_layer[1]()\n",
        "\n",
        "        if final_activation and act_layer != nn.Identity:\n",
        "            gain_factor = 2.0\n",
        "        else:\n",
        "            gain_factor = 1.0\n",
        "\n",
        "        if outer_skip == \"linear\":\n",
        "            self.outer_skip = nn.Conv2d(embed_dim, embed_dim, 1, 1, bias=False)\n",
        "            gain_factor /= 2.0\n",
        "            torch.nn.init.normal_(self.outer_skip.weight, std=math.sqrt(gain_factor / embed_dim))\n",
        "        elif outer_skip == \"identity\":\n",
        "            self.outer_skip = nn.Identity()\n",
        "            gain_factor /= 2.0\n",
        "        elif outer_skip == \"none\":\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown skip connection type {outer_skip}\")\n",
        "\n",
        "        if use_mlp == True:\n",
        "            MLPH = DistributedMLP if (comm.get_size(\"matmul\") > 1) else MLP\n",
        "            mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "            self.mlp = MLPH(\n",
        "                in_features=embed_dim,\n",
        "                hidden_features=mlp_hidden_dim,\n",
        "                act_layer=act_layer,\n",
        "                drop_rate=mlp_drop_rate,\n",
        "                drop_type=\"features\",\n",
        "                comm_inp_name=comm_feature_inp_name,\n",
        "                comm_hidden_name=comm_feature_hidden_name,\n",
        "                checkpointing=checkpointing,\n",
        "                gain=gain_factor,\n",
        "            )\n",
        "\n",
        "        # dropout\n",
        "        self.drop_path = DropPath(path_drop_rate) if path_drop_rate > 0.0 else nn.Identity()\n",
        "\n",
        "        if final_activation:\n",
        "            self.act_layer1 = act_layer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Updated FNO block\n",
        "        \"\"\"\n",
        "\n",
        "        x, residual = self.filter(x)\n",
        "\n",
        "        x = self.norm0(x)\n",
        "\n",
        "        if hasattr(self, \"inner_skip\"):\n",
        "            x = x + self.inner_skip(residual)\n",
        "\n",
        "        if hasattr(self, \"act_layer0\"):\n",
        "            x = self.act_layer0(x)\n",
        "\n",
        "        if hasattr(self, \"mlp\"):\n",
        "            x = self.mlp(x)\n",
        "\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        x = self.drop_path(x)\n",
        "\n",
        "        if hasattr(self, \"outer_skip\"):\n",
        "            x = x + self.outer_skip(residual)\n",
        "\n",
        "        if hasattr(self, \"act_layer1\"):\n",
        "            x = self.act_layer1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SphericalFourierNeuralOperatorNet(nn.Module):\n",
        "    \"\"\"\n",
        "    SFNO implementation as in Bonev et al.; Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spectral_transform=\"sht\",\n",
        "        model_grid_type=\"equiangular\",\n",
        "        sht_grid_type=\"legendre-gauss\",\n",
        "        filter_type=\"linear\",\n",
        "        operator_type=\"dhconv\",\n",
        "        inp_shape=(721, 1440),\n",
        "        out_shape=(721, 1440),\n",
        "        scale_factor=8,\n",
        "        inp_chans=2,\n",
        "        out_chans=2,\n",
        "        embed_dim=32,\n",
        "        num_layers=4,\n",
        "        repeat_layers=1,\n",
        "        use_mlp=True,\n",
        "        mlp_ratio=2.0,\n",
        "        encoder_ratio=1,\n",
        "        decoder_ratio=1,\n",
        "        activation_function=\"gelu\",\n",
        "        encoder_layers=1,\n",
        "        pos_embed=\"none\",\n",
        "        pos_drop_rate=0.0,\n",
        "        path_drop_rate=0.0,\n",
        "        mlp_drop_rate=0.0,\n",
        "        normalization_layer=\"instance_norm\",\n",
        "        max_modes=None,\n",
        "        hard_thresholding_fraction=1.0,\n",
        "        big_skip=True,\n",
        "        rank=1.0,\n",
        "        factorization=None,\n",
        "        separable=False,\n",
        "        complex_activation=\"real\",\n",
        "        spectral_layers=3,\n",
        "        bias=False,\n",
        "        checkpointing=0,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(SphericalFourierNeuralOperatorNet, self).__init__()\n",
        "\n",
        "        self.inp_shape = inp_shape\n",
        "        self.out_shape = out_shape\n",
        "        self.inp_chans = inp_chans\n",
        "        self.out_chans = out_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.repeat_layers = repeat_layers\n",
        "        self.big_skip = big_skip\n",
        "        self.checkpointing = checkpointing\n",
        "\n",
        "        # compute the downscaled image size\n",
        "        self.h = int(self.inp_shape[0] // scale_factor)\n",
        "        self.w = int(self.inp_shape[1] // scale_factor)\n",
        "\n",
        "        # initialize spectral transforms\n",
        "        self._init_spectral_transforms(spectral_transform, model_grid_type, sht_grid_type, hard_thresholding_fraction, max_modes)\n",
        "\n",
        "        # determine activation function\n",
        "        if activation_function == \"relu\":\n",
        "            activation_function = nn.ReLU\n",
        "        elif activation_function == \"gelu\":\n",
        "            activation_function = nn.GELU\n",
        "        elif activation_function == \"silu\":\n",
        "            activation_function = nn.SiLU\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function {activation_function}\")\n",
        "\n",
        "        # set up encoder\n",
        "        if comm.get_size(\"matmul\") > 1:\n",
        "            self.encoder = DistributedEncoderDecoder(\n",
        "                num_layers=encoder_layers,\n",
        "                input_dim=self.inp_chans,\n",
        "                output_dim=self.embed_dim,\n",
        "                hidden_dim=int(encoder_ratio * self.embed_dim),\n",
        "                act_layer=activation_function,\n",
        "                input_format=\"nchw\",\n",
        "                comm_inp_name=\"fin\",\n",
        "                comm_out_name=\"fout\",\n",
        "            )\n",
        "            fblock_mlp_inp_name = self.encoder.comm_out_name\n",
        "            fblock_mlp_hidden_name = \"fout\" if (self.encoder.comm_out_name == \"fin\") else \"fin\"\n",
        "        else:\n",
        "            self.encoder = EncoderDecoder(\n",
        "                num_layers=encoder_layers,\n",
        "                input_dim=self.inp_chans,\n",
        "                output_dim=self.embed_dim,\n",
        "                hidden_dim=int(encoder_ratio * self.embed_dim),\n",
        "                act_layer=activation_function,\n",
        "                input_format=\"nchw\",\n",
        "            )\n",
        "            fblock_mlp_inp_name = \"fin\"\n",
        "            fblock_mlp_hidden_name = \"fout\"\n",
        "\n",
        "        # dropout\n",
        "        self.pos_drop = nn.Dropout(p=pos_drop_rate) if pos_drop_rate > 0.0 else nn.Identity()\n",
        "        dpr = [x.item() for x in torch.linspace(0, path_drop_rate, num_layers)]\n",
        "\n",
        "        # pick norm layer\n",
        "        if normalization_layer == \"layer_norm\":\n",
        "            norm_layer_inp = partial(DistributedLayerNorm, normalized_shape=(embed_dim), elementwise_affine=True, eps=1e-6)\n",
        "            norm_layer_out = norm_layer_mid = norm_layer_inp\n",
        "        elif normalization_layer == \"instance_norm\":\n",
        "            if comm.get_size(\"spatial\") > 1:\n",
        "                norm_layer_inp = partial(DistributedInstanceNorm2d, num_features=embed_dim, eps=1e-6, affine=True)\n",
        "            else:\n",
        "                norm_layer_inp = partial(nn.InstanceNorm2d, num_features=embed_dim, eps=1e-6, affine=True, track_running_stats=False)\n",
        "            norm_layer_out = norm_layer_mid = norm_layer_inp\n",
        "        elif normalization_layer == \"none\":\n",
        "            norm_layer_out = norm_layer_mid = norm_layer_inp = nn.Identity\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Error, normalization {normalization_layer} not implemented.\")\n",
        "\n",
        "        # FNO blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for i in range(num_layers):\n",
        "            first_layer = i == 0\n",
        "            last_layer = i == num_layers - 1\n",
        "\n",
        "            forward_transform = self.trans_down if first_layer else self.trans\n",
        "            inverse_transform = self.itrans_up if last_layer else self.itrans\n",
        "\n",
        "            inner_skip = \"none\"\n",
        "            outer_skip = \"linear\"\n",
        "\n",
        "            if first_layer:\n",
        "                norm_layer = (norm_layer_inp, norm_layer_mid)\n",
        "            elif last_layer:\n",
        "                norm_layer = (norm_layer_mid, norm_layer_out)\n",
        "            else:\n",
        "                norm_layer = (norm_layer_mid, norm_layer_mid)\n",
        "\n",
        "            block = FourierNeuralOperatorBlock(\n",
        "                forward_transform,\n",
        "                inverse_transform,\n",
        "                embed_dim,\n",
        "                filter_type=filter_type,\n",
        "                operator_type=operator_type,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                mlp_drop_rate=mlp_drop_rate,\n",
        "                path_drop_rate=dpr[i],\n",
        "                act_layer=activation_function,\n",
        "                norm_layer=norm_layer,\n",
        "                inner_skip=inner_skip,\n",
        "                outer_skip=outer_skip,\n",
        "                use_mlp=use_mlp,\n",
        "                comm_feature_inp_name=fblock_mlp_inp_name,\n",
        "                comm_feature_hidden_name=fblock_mlp_hidden_name,\n",
        "                rank=rank,\n",
        "                factorization=factorization,\n",
        "                separable=separable,\n",
        "                complex_activation=complex_activation,\n",
        "                spectral_layers=spectral_layers,\n",
        "                bias=bias,\n",
        "                checkpointing=checkpointing,\n",
        "            )\n",
        "\n",
        "            self.blocks.append(block)\n",
        "\n",
        "        # decoder takes the output of FNO blocks and the residual from the big skip connection\n",
        "        if comm.get_size(\"matmul\") > 1:\n",
        "            comm_inp_name = fblock_mlp_inp_name\n",
        "            comm_out_name = fblock_mlp_hidden_name\n",
        "            self.decoder = DistributedEncoderDecoder(\n",
        "                num_layers=encoder_layers,\n",
        "                input_dim=embed_dim,\n",
        "                output_dim=self.out_chans,\n",
        "                hidden_dim=int(decoder_ratio * embed_dim),\n",
        "                act_layer=activation_function,\n",
        "                gain=0.5 if self.big_skip else 1.0,\n",
        "                comm_inp_name=comm_inp_name,\n",
        "                comm_out_name=comm_out_name,\n",
        "                input_format=\"nchw\",\n",
        "            )\n",
        "            self.gather_shapes = compute_split_shapes(self.out_chans,\n",
        "                                                      comm.get_size(self.decoder.comm_out_name))\n",
        "\n",
        "        else:\n",
        "            self.decoder = EncoderDecoder(\n",
        "                num_layers=encoder_layers,\n",
        "                input_dim=embed_dim,\n",
        "                output_dim=self.out_chans,\n",
        "                hidden_dim=int(decoder_ratio * embed_dim),\n",
        "                act_layer=activation_function,\n",
        "                gain=0.5 if self.big_skip else 1.0,\n",
        "                input_format=\"nchw\",\n",
        "            )\n",
        "\n",
        "        # output transform\n",
        "        if self.big_skip:\n",
        "            self.residual_transform = nn.Conv2d(self.inp_chans, self.out_chans, 1, bias=False)\n",
        "            self.residual_transform.weight.is_shared_mp = [\"spatial\"]\n",
        "            self.residual_transform.weight.sharded_dims_mp = [None, None, None, None]\n",
        "            scale = math.sqrt(0.5 / self.inp_chans)\n",
        "            nn.init.normal_(self.residual_transform.weight, mean=0.0, std=scale)\n",
        "\n",
        "        # learned position embedding\n",
        "        if pos_embed == \"direct\":\n",
        "            # currently using deliberately a differently shape position embedding\n",
        "            self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, self.inp_shape_loc[0], self.inp_shape_loc[1]))\n",
        "            # information about how tensors are shared / sharded across ranks\n",
        "            self.pos_embed.is_shared_mp = []  # no reduction required since pos_embed is already serial\n",
        "            self.pos_embed.sharded_dims_mp = [None, None, \"h\", \"w\"]\n",
        "            self.pos_embed.type = \"direct\"\n",
        "            with torch.no_grad():\n",
        "                nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        elif pos_embed == \"frequency\":\n",
        "            if comm.get_size(\"spatial\") > 1:\n",
        "                lmax_loc = self.itrans_up.l_shapes[comm.get_rank(\"h\")]\n",
        "                mmax_loc = self.itrans_up.m_shapes[comm.get_rank(\"w\")]\n",
        "            else:\n",
        "                lmax_loc = self.itrans_up.lmax\n",
        "                mmax_loc = self.itrans_up.mmax\n",
        "\n",
        "            rcoeffs = nn.Parameter(torch.tril(torch.randn(1, embed_dim, lmax_loc, mmax_loc), diagonal=0))\n",
        "            ccoeffs = nn.Parameter(torch.tril(torch.randn(1, embed_dim, lmax_loc, mmax_loc - 1), diagonal=-1))\n",
        "            with torch.no_grad():\n",
        "                nn.init.trunc_normal_(rcoeffs, std=0.02)\n",
        "                nn.init.trunc_normal_(ccoeffs, std=0.02)\n",
        "            self.pos_embed = nn.ParameterList([rcoeffs, ccoeffs])\n",
        "            self.pos_embed.type = \"frequency\"\n",
        "            self.pos_embed.is_shared_mp = []\n",
        "            self.pos_embed.sharded_dims_mp = [None, None, \"h\", \"w\"]\n",
        "\n",
        "        elif pos_embed == \"none\" or pos_embed == \"None\" or pos_embed == None:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"Unknown position embedding type\")\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def _init_spectral_transforms(\n",
        "        self,\n",
        "        spectral_transform=\"sht\",\n",
        "        model_grid_type=\"equiangular\",\n",
        "        sht_grid_type=\"legendre-gauss\",\n",
        "        hard_thresholding_fraction=1.0,\n",
        "        max_modes=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the spectral transforms based on the maximum number of modes to keep. Handles the computation\n",
        "        of local image shapes and domain parallelism, based on the\n",
        "        \"\"\"\n",
        "\n",
        "        if max_modes is not None:\n",
        "            modes_lat, modes_lon = max_modes\n",
        "        else:\n",
        "            modes_lat = int(self.h * hard_thresholding_fraction)\n",
        "            modes_lon = int((self.w // 2 + 1) * hard_thresholding_fraction)\n",
        "\n",
        "        # prepare the spectral transforms\n",
        "        if spectral_transform == \"sht\":\n",
        "            sht_handle = th.RealSHT\n",
        "            isht_handle = th.InverseRealSHT\n",
        "\n",
        "            # parallelism\n",
        "            if comm.get_size(\"spatial\") > 1:\n",
        "                polar_group = None if (comm.get_size(\"h\") == 1) else comm.get_group(\"h\")\n",
        "                azimuth_group = None if (comm.get_size(\"w\") == 1) else comm.get_group(\"w\")\n",
        "                thd.init(polar_group, azimuth_group)\n",
        "                sht_handle = thd.DistributedRealSHT\n",
        "                isht_handle = thd.DistributedInverseRealSHT\n",
        "\n",
        "            # set up\n",
        "            self.trans_down = sht_handle(*self.inp_shape, lmax=modes_lat, mmax=modes_lon, grid=model_grid_type).float()\n",
        "            self.itrans_up = isht_handle(*self.out_shape, lmax=modes_lat, mmax=modes_lon, grid=model_grid_type).float()\n",
        "            self.trans = sht_handle(self.h, self.w, lmax=modes_lat, mmax=modes_lon, grid=sht_grid_type).float()\n",
        "            self.itrans = isht_handle(self.h, self.w, lmax=modes_lat, mmax=modes_lon, grid=sht_grid_type).float()\n",
        "\n",
        "        elif spectral_transform == \"fft\":\n",
        "            fft_handle = RealFFT2\n",
        "            ifft_handle = InverseRealFFT2\n",
        "\n",
        "            if comm.get_size(\"spatial\") > 1:\n",
        "                h_group = None if (comm.get_size(\"h\") == 1) else comm.get_group(\"h\")\n",
        "                w_group = None if (comm.get_size(\"w\") == 1) else comm.get_group(\"w\")\n",
        "                thd.init(h_group, w_group)\n",
        "                fft_handle = DistributedRealFFT2\n",
        "                ifft_handle = DistributedInverseRealFFT2\n",
        "\n",
        "            self.trans_down = fft_handle(*self.inp_shape, lmax=modes_lat, mmax=modes_lon).float()\n",
        "            self.itrans_up = ifft_handle(*self.out_shape, lmax=modes_lat, mmax=modes_lon).float()\n",
        "            self.trans = fft_handle(self.h, self.w, lmax=modes_lat, mmax=modes_lon).float()\n",
        "            self.itrans = ifft_handle(self.h, self.w, lmax=modes_lat, mmax=modes_lon).float()\n",
        "        else:\n",
        "            raise (ValueError(\"Unknown spectral transform\"))\n",
        "\n",
        "        # use the SHT/FFT to compute the local, downscaled grid dimensions\n",
        "        if comm.get_size(\"spatial\") > 1:\n",
        "            self.inp_shape_loc = (self.trans_down.lat_shapes[comm.get_rank(\"h\")],\n",
        "                                  self.trans_down.lon_shapes[comm.get_rank(\"w\")])\n",
        "            self.out_shape_loc = (self.itrans_up.lat_shapes[comm.get_rank(\"h\")],\n",
        "                                  self.itrans_up.lon_shapes[comm.get_rank(\"w\")])\n",
        "            self.h_loc = self.itrans.lat_shapes[comm.get_rank(\"h\")]\n",
        "            self.w_loc = self.itrans.lon_shapes[comm.get_rank(\"w\")]\n",
        "        else:\n",
        "            self.inp_shape_loc = (self.trans_down.nlat, self.trans_down.nlon)\n",
        "            self.out_shape_loc = (self.itrans_up.nlat, self.itrans_up.nlon)\n",
        "            self.h_loc = self.itrans.nlat\n",
        "            self.w_loc = self.itrans.nlon\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {\"pos_embed\", \"cls_token\"}\n",
        "\n",
        "    def _forward_features(self, x):\n",
        "        for r in range(self.repeat_layers):\n",
        "            for blk in self.blocks:\n",
        "                if self.checkpointing >= 3:\n",
        "                    x = checkpoint(blk, x, use_reentrant=False)\n",
        "                else:\n",
        "                    x = blk(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # save big skip\n",
        "        if self.big_skip:\n",
        "            # if output shape differs, use the spectral transforms to change resolution\n",
        "            if self.out_shape != self.inp_shape:\n",
        "                xtype = x.dtype\n",
        "                # only take the predicted channels as residual\n",
        "                residual = x.to(torch.float32)\n",
        "                with amp.autocast(enabled=False):\n",
        "                    residual = self.trans_down(residual)\n",
        "                    residual = residual.contiguous()\n",
        "                    residual = self.itrans_up(residual)\n",
        "                    residual = residual.to(dtype=xtype)\n",
        "            else:\n",
        "                # only take the predicted channels\n",
        "                residual = x\n",
        "\n",
        "        if comm.get_size(\"fin\") > 1:\n",
        "            x = scatter_to_parallel_region(x, 1, \"fin\")\n",
        "\n",
        "        if self.checkpointing >= 1:\n",
        "            x = checkpoint(self.encoder, x, use_reentrant=False)\n",
        "        else:\n",
        "            x = self.encoder(x)\n",
        "\n",
        "        if hasattr(self, \"pos_embed\"):\n",
        "            if self.pos_embed.type == \"frequency\":\n",
        "                pos_embed = torch.stack([self.pos_embed[0], nn.functional.pad(self.pos_embed[1], (1, 0), \"constant\", 0)], dim=-1)\n",
        "                with amp.autocast(enabled=False):\n",
        "                    pos_embed = self.itrans_up(torch.view_as_complex(pos_embed))\n",
        "            else:\n",
        "                pos_embed = self.pos_embed\n",
        "\n",
        "            # add pos embed\n",
        "            x = x + pos_embed\n",
        "\n",
        "        # maybe clean the padding just in case\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        # do the feature extraction\n",
        "        x = self._forward_features(x)\n",
        "\n",
        "        if self.checkpointing >= 1:\n",
        "            x = checkpoint(self.decoder, x, use_reentrant=False)\n",
        "        else:\n",
        "            x = self.decoder(x)\n",
        "\n",
        "        if hasattr(self.decoder, \"comm_out_name\") and (comm.get_size(self.decoder.comm_out_name) > 1):\n",
        "            x = gather_from_parallel_region(x, 1, self.gather_shapes, self.decoder.comm_out_name)\n",
        "\n",
        "        if self.big_skip:\n",
        "            x = x + self.residual_transform(residual)\n",
        "\n",
        "        return x\n",
        "\n",
        "# this part exposes the model to modulus by constructing modulus Modules\n",
        "\n",
        "class SphericalFourierNeuralOperatorNetMetaData(ModelMetaData):\n",
        "    name: str = \"SFNO\"\n",
        "\n",
        "    jit: bool = False\n",
        "    cuda_graphs: bool = False\n",
        "    amp_cpu: bool = False\n",
        "    amp_gpu: bool = True\n",
        "\n",
        "# SFNO = modulus.Module.from_torch(\n",
        "#     SphericalFourierNeuralOperatorNet,\n",
        "#     SphericalFourierNeuralOperatorNetMetaData()\n",
        "# )\n",
        "\n",
        "# class FourierNeuralOperatorNet(SphericalFourierNeuralOperatorNet):\n",
        "#     def __init__(self, *args, **kwargs):\n",
        "#         return super().__init__(*args, spectral_transform=\"fft\", **kwargs)\n",
        "\n",
        "\n",
        "# class FourierNeuralOperatorNetMetaData(ModelMetaData):\n",
        "#     name: str = \"FNO\"\n",
        "\n",
        "#     jit: bool = False\n",
        "#     cuda_graphs: bool = False\n",
        "#     amp_cpu: bool = False\n",
        "#     amp_gpu: bool = True\n",
        "\n",
        "# FNO = modulus.Module.from_torch(\n",
        "#     FourierNeuralOperatorNet,\n",
        "#     FourierNeuralOperatorNetMetaData()\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezhpzXvxNJIg"
      },
      "outputs": [],
      "source": [
        "    # architecture related\n",
        "model = SphericalFourierNeuralOperatorNet(nettype = \"SFNO\",\n",
        "                                          model_grid_type = \"equiangular\",\n",
        "                                          sht_grid_type = \"legendre-gauss\",\n",
        "                                          filter_type = \"linear\",\n",
        "                                          scale_factor = 3,\n",
        "                                          embed_dim = 384,\n",
        "                                          num_layers = 8,\n",
        "                                          complex_activation = \"real\",\n",
        "                                          normalization_layer = \"instance_norm\",\n",
        "                                          hard_thresholding_fraction = 1.0,\n",
        "                                          use_mlp =   True,\n",
        "                                          mlp_mode = \"serial\",\n",
        "                                          mlp_ratio = 2,\n",
        "                                          separable = False,\n",
        "                                          operator_type = \"dhconv\",\n",
        "                                          activation_function = \"gelu\",\n",
        "                                          pos_embed =\"none\",\n",
        "                                          inp_shape=(721, 1440),\n",
        "                                          out_shape=(721, 1440),\n",
        "                                          inp_chans=2,\n",
        "                                          out_chans=2,) # \"none\", \"direct\" or \"freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNj9m7e0ZPmq",
        "outputId": "9f3fff6e-225a-46f8-8e37-b7de92c6b4be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/content/makani/models/common/spectral_convolution.py:130: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(enabled=False):\n",
            "\n",
            "WARNING:py.warnings:/content/makani/models/common/spectral_convolution.py:140: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(enabled=False):\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nlat = 721\n",
        "nlon = 1440\n",
        "model(torch.randn(1, 2, 721, 1440)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPTNzjcIY0OV"
      },
      "source": [
        "# AFNO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOaXfD75Pfpj"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from copy import Error, deepcopy\n",
        "from re import S\n",
        "from numpy.lib.arraypad import pad\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "from torch.nn.modules.container import Sequential\n",
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "from typing import Optional\n",
        "import math\n",
        "\n",
        "# helpers\n",
        "from makani.models.common import ComplexReLU, PatchEmbed, DropPath, MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA_KEVESY3oO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}